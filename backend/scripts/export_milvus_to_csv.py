"""
å°†æœ¬åœ° Milvus å‘é‡æ•°æ®åº“çš„æ•°æ®å¯¼å‡ºä¸º CSV æ–‡ä»¶

ç”¨æ³•:
    python export_milvus_to_csv.py
    python export_milvus_to_csv.py --output-dir ./exports
    python export_milvus_to_csv.py --collections rag_standard rag_faq
    python export_milvus_to_csv.py --include-vector  # åŒ…å«å‘é‡å­—æ®µ
"""
import sys
import argparse
import csv
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# tqdm å¯é€‰ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # å¦‚æœæ²¡æœ‰ tqdmï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½ç¬¦
    class tqdm:
        def __init__(self, *args, **kwargs):
            pass
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
        def update(self, n=1):
            pass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.config import settings
from pymilvus import MilvusClient


class MilvusExporter:
    """Milvus æ•°æ®å¯¼å‡ºç±»"""
    
    def __init__(self, local_host: str = None, local_port: str = None):
        """
        åˆå§‹åŒ–å¯¼å‡ºå™¨
        
        Args:
            local_host: æœ¬åœ° Milvus ä¸»æœºåœ°å€
            local_port: æœ¬åœ° Milvus ç«¯å£
        """
        # æœ¬åœ°é…ç½®
        self.local_host = local_host or settings.MILVUS_HOST
        self.local_port = local_port or settings.MILVUS_PORT
        self.local_client = None
        
        # é›†åˆé…ç½®
        self.rag_collections = [
            settings.COLLECTION_STANDARD,
            settings.COLLECTION_KNOWLEDGE,
            settings.COLLECTION_INTERNAL,
            settings.COLLECTION_PERSONAL
        ]
        self.faq_collection = settings.COLLECTION_FAQ
    
    def connect(self):
        """è¿æ¥åˆ°æœ¬åœ° Milvus"""
        print(f"\n{'='*80}")
        print("ğŸ”Œ æ­£åœ¨è¿æ¥ Milvus...")
        print(f"{'='*80}")
        
        # è¿æ¥æœ¬åœ°
        try:
            local_uri = f"http://{self.local_host}:{self.local_port}"
            print(f"ğŸ“¡ è¿æ¥æœ¬åœ° Milvus: {local_uri}")
            self.local_client = MilvusClient(uri=local_uri)
            local_cols = self.local_client.list_collections()
            print(f"âœ… æœ¬åœ°è¿æ¥æˆåŠŸï¼Œæ‰¾åˆ° {len(local_cols)} ä¸ªé›†åˆ: {local_cols}")
        except Exception as e:
            print(f"âŒ æœ¬åœ°è¿æ¥å¤±è´¥: {e}")
            raise
    
    def get_collection_stats(self, collection_name: str) -> int:
        """è·å–é›†åˆä¸­çš„è®°å½•æ•°é‡ï¼Œå¤±è´¥æ—¶è¿”å› 0"""
        try:
            # ä¼˜å…ˆå°è¯•å®˜æ–¹ç»Ÿè®¡æ¥å£ï¼ˆéƒ¨åˆ†ç‰ˆæœ¬æ²¡æœ‰ï¼‰
            if hasattr(self.local_client, "get_collection_stats"):
                stats = self.local_client.get_collection_stats(collection_name)
                return stats.get("row_count", 0) or stats.get("rowCount", 0) or 0
            if hasattr(self.local_client, "get_collection_statistics"):
                stats = self.local_client.get_collection_statistics(collection_name)
                return stats.get("row_count", 0) or stats.get("rowCount", 0) or 0
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šä½¿ç”¨ count(*) æŸ¥è¯¢
            res = self.local_client.query(
                collection_name=collection_name,
                filter="",
                output_fields=["count(*)"],
            )
            if res and isinstance(res, list) and "count(*)" in res[0]:
                return int(res[0]["count(*)"])
            print("  âš ï¸  å½“å‰å®¢æˆ·ç«¯ä¸æ”¯æŒç»Ÿè®¡æ¥å£ï¼Œcount(*) ä¹Ÿæœªè¿”å›ï¼Œç»§ç»­å°è¯•ç›´æ¥è¯»å–æ•°æ®")
            return 0
        except Exception as e:
            print(f"  âš ï¸  æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯: {e}")
            return 0
    
    def read_collection_data(self, collection_name: str, batch_size: int = 1000) -> List[Dict[str, Any]]:
        """
        ä»æœ¬åœ°é›†åˆè¯»å–æ‰€æœ‰æ•°æ®
        
        Args:
            collection_name: é›†åˆåç§°
            batch_size: æ¯æ‰¹è¯»å–çš„æ•°é‡
            
        Returns:
            æ‰€æœ‰æ•°æ®çš„åˆ—è¡¨
        """
        print(f"\nğŸ“– æ­£åœ¨è¯»å–é›†åˆ {collection_name} çš„æ•°æ®...")
        
        # åˆ¤æ–­é›†åˆç±»å‹
        is_faq = collection_name == self.faq_collection
        
        # ç¡®å®šè¾“å‡ºå­—æ®µ
        if is_faq:
            output_fields = ["id", "vector", "question", "answer", "source"]
        else:
            output_fields = ["id", "vector", "text", "source", "dept_id", "user_id"]
        
        # è·å–æ€»æ•°ï¼ˆå¯èƒ½å¤±è´¥è¿”å› 0ï¼‰
        total_count = self.get_collection_stats(collection_name)
        if total_count > 0:
            print(f"  ğŸ“Š æ€»è®°å½•æ•°: {total_count}")
        else:
            print("  âš ï¸  æ— æ³•è·å–æ€»æ•°æˆ–æ€»æ•°ä¸º 0ï¼Œç›´æ¥å°è¯•è¯»å–æ•°æ®")
        
        # åˆ†æ‰¹è¯»å–æ•°æ®
        all_data = []
        max_limit = 16384  # Milvus é»˜è®¤æœ€å¤§ limit
        
        try:
            # å…ˆå°è¯•ä¸€æ¬¡æ€§è¯»å–ï¼ˆæœ€å¤š 16384 æ¡ï¼‰ï¼Œå½“å‰æ•°æ®é‡è¶³å¤Ÿ
            primary_limit = max_limit if total_count == 0 else min(total_count, max_limit)
            results = self.local_client.query(
                collection_name=collection_name,
                filter="",
                limit=primary_limit,
                output_fields=output_fields
            )
            all_data = results or []
            if not all_data:
                print(f"  âš ï¸  é›†åˆ {collection_name} ä¸­æ²¡æœ‰æ•°æ®")
                return []
            print(f"  âœ… æˆåŠŸè¯»å– {len(all_data)} æ¡è®°å½•")
            
            # å¦‚æœä¼°è®¡çš„æ€»æ•°å¤§äº max_limitï¼Œåˆ™ç»§ç»­åˆ†æ‰¹è¯»å–å‰©ä½™æ•°æ®
            if total_count > max_limit:
                print(f"  ğŸ“¦ æ•°æ®é‡è¾ƒå¤§ï¼Œç»§ç»­åˆ†æ‰¹è¯»å–ï¼ˆæ¯æ‰¹æœ€å¤š {batch_size} æ¡ï¼‰...")
                last_max_id = max(r["id"] for r in all_data)
                read_count = len(all_data)
                
                with tqdm(total=total_count, desc=f"  è¯»å– {collection_name}") if HAS_TQDM else tqdm() as pbar:
                    while read_count < total_count:
                        filter_expr = f"id > {last_max_id}"
                        batch_limit = min(batch_size, max_limit)
                        results = self.local_client.query(
                            collection_name=collection_name,
                            filter=filter_expr,
                            limit=batch_limit,
                            output_fields=output_fields
                        )
                        
                        if not results:
                            break
                        
                        all_data.extend(results)
                        read_count += len(results)
                        
                        if HAS_TQDM:
                            pbar.update(len(results))
                        
                        last_max_id = max(r["id"] for r in results)
                        
                        if len(results) < batch_limit:
                            break
                
                print(f"  âœ… åˆ†æ‰¹è¯»å–å®Œæˆï¼Œå…± {len(all_data)} æ¡è®°å½•")
        except Exception as e:
            print(f"  âŒ è¯»å–æ•°æ®æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []
        
        return all_data
    
    def format_vector(self, vector: List[float]) -> str:
        """å°†å‘é‡æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²"""
        if not vector:
            return ""
        # å°†å‘é‡è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ï¼Œç¡®ä¿å¯åºåˆ—åŒ–ä¸º float
        try:
            vector_as_float = [float(x) for x in vector]
        except Exception:
            vector_as_float = [float(x.item()) if hasattr(x, "item") else float(x) for x in vector]
        return json.dumps(vector_as_float)

    def _sanitize_value(self, value: Any) -> Any:
        """å°†å€¼è½¬æ¢ä¸ºå¯ JSON åºåˆ—åŒ–çš„åŸºæœ¬ç±»å‹"""
        if value is None:
            return ""
        if isinstance(value, (str, int, float)):
            return value
        if hasattr(value, "item"):  # numpy ç±»å‹
            try:
                return value.item()
            except Exception:
                return float(value)
        if isinstance(value, list):
            return [self._sanitize_value(v) for v in value]
        if isinstance(value, dict):
            return {k: self._sanitize_value(v) for k, v in value.items()}
        return str(value)
    
    def prepare_data_for_csv(
        self,
        data: List[Dict[str, Any]],
        include_vector: bool = False,
        for_import: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        å‡†å¤‡æ•°æ®ç”¨äº CSV å¯¼å‡º
        
        Args:
            data: åŸå§‹æ•°æ®åˆ—è¡¨
            include_vector: æ˜¯å¦åŒ…å«å‘é‡å­—æ®µ
            
        Returns:
            å¤„ç†åçš„æ•°æ®åˆ—è¡¨
        """
        prepared = []
        for item in data:
            new_item = {}
            for key, value in item.items():
                if key == "id" and for_import:
                    # å¯¼å…¥æ¨¡å¼ä¸‹å»æ‰è‡ªå¢ä¸»é”®
                    continue
                if key == "vector":
                    if include_vector:
                        # åŒ…å«å‘é‡ï¼šè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
                        new_item["vector"] = self.format_vector(value)
                    else:
                        # ä¸åŒ…å«å‘é‡ï¼šè·³è¿‡
                        continue
                elif isinstance(value, (list, dict)):
                    # å…¶ä»–å¤æ‚ç±»å‹ä¹Ÿè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
                    sanitized = self._sanitize_value(value)
                    new_item[key] = json.dumps(sanitized, ensure_ascii=False) if sanitized != "" else ""
                elif value is None:
                    new_item[key] = ""
                else:
                    new_item[key] = self._sanitize_value(value)
            prepared.append(new_item)
        return prepared
    
    def export_to_csv(
        self,
        collection_name: str,
        output_dir: Path,
        include_vector: bool = False,
        for_import: bool = False,
    ):
        """
        å°†é›†åˆæ•°æ®å¯¼å‡ºä¸º CSV æ–‡ä»¶
        
        Args:
            collection_name: é›†åˆåç§°
            output_dir: è¾“å‡ºç›®å½•
            include_vector: æ˜¯å¦åŒ…å«å‘é‡å­—æ®µ
        """
        # è¯»å–æ•°æ®
        data = self.read_collection_data(collection_name)
        if not data:
            print(f"  âš ï¸  é›†åˆ {collection_name} æ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡å¯¼å‡º")
            return
        
        # å‡†å¤‡æ•°æ®
        print(f"\nğŸ“ æ­£åœ¨å‡†å¤‡æ•°æ®...")
        prepared_data = self.prepare_data_for_csv(
            data,
            include_vector=include_vector,
            for_import=for_import,
        )
        
        if not prepared_data:
            print(f"  âš ï¸  æ²¡æœ‰æ•°æ®éœ€è¦å¯¼å‡º")
            return
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"{collection_name}_{timestamp}.csv"
        
        # è·å–å­—æ®µåï¼ˆæ’é™¤ vector å¦‚æœä¸åŒ…å«ï¼‰
        if collection_name == self.faq_collection:
            field_order = ["question", "answer", "source"]
        else:
            field_order = ["text", "source", "dept_id", "user_id"]
        if include_vector:
            field_order.insert(0, "vector")
        # ä¿ç•™æœªçŸ¥å­—æ®µï¼ˆå®‰å…¨å…œåº•ï¼‰
        extra_fields = [f for f in prepared_data[0].keys() if f not in field_order]
        fieldnames = field_order + extra_fields
        
        # å†™å…¥ CSV
        print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡ºåˆ° {output_file}...")
        try:
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                with tqdm(total=len(prepared_data), desc=f"  å†™å…¥ {collection_name}") if HAS_TQDM else tqdm() as pbar:
                    for row in prepared_data:
                        writer.writerow(row)
                        if HAS_TQDM:
                            pbar.update(1)
            
            file_size = output_file.stat().st_size / 1024 / 1024  # MB
            print(f"  âœ… å¯¼å‡ºæˆåŠŸï¼")
            print(f"  ğŸ“„ æ–‡ä»¶: {output_file}")
            print(f"  ğŸ“Š è®°å½•æ•°: {len(prepared_data)}")
            print(f"  ğŸ’¾ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        except Exception as e:
            print(f"  âŒ å¯¼å‡ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def export_all(
        self,
        output_dir: Path,
        collections: Optional[List[str]] = None,
        include_vector: bool = False,
        for_import: bool = False,
    ):
        """
        å¯¼å‡ºæ‰€æœ‰æˆ–æŒ‡å®šçš„é›†åˆ
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            collections: è¦å¯¼å‡ºçš„é›†åˆåˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™å¯¼å‡ºæ‰€æœ‰é›†åˆ
            include_vector: æ˜¯å¦åŒ…å«å‘é‡å­—æ®µ
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¡®å®šè¦å¯¼å‡ºçš„é›†åˆ
        if collections is None:
            # å¯¼å‡ºæ‰€æœ‰é›†åˆ
            all_collections = self.rag_collections + [self.faq_collection]
            existing_collections = self.local_client.list_collections()
            collections = [col for col in all_collections if col in existing_collections]
        
        if not collections:
            print("  âš ï¸  æ²¡æœ‰æ‰¾åˆ°è¦å¯¼å‡ºçš„é›†åˆ")
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ“¤ å¼€å§‹å¯¼å‡º {len(collections)} ä¸ªé›†åˆåˆ° {output_dir}")
        print(f"{'='*80}")
        
        for i, collection_name in enumerate(collections, 1):
            print(f"\n[{i}/{len(collections)}] å¤„ç†é›†åˆ: {collection_name}")
            try:
                self.export_to_csv(
                    collection_name,
                    output_dir,
                    include_vector=include_vector,
                    for_import=for_import,
                )
            except Exception as e:
                print(f"  âŒ å¯¼å‡ºé›†åˆ {collection_name} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"\n{'='*80}")
        print("âœ… å¯¼å‡ºå®Œæˆï¼")
        print(f"{'='*80}")


def main():
    parser = argparse.ArgumentParser(
        description="å°†æœ¬åœ° Milvus å‘é‡æ•°æ®åº“çš„æ•°æ®å¯¼å‡ºä¸º CSV æ–‡ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯¼å‡ºæ‰€æœ‰é›†åˆåˆ°é»˜è®¤ç›®å½•
  python export_milvus_to_csv.py
  
  # å¯¼å‡ºåˆ°æŒ‡å®šç›®å½•
  python export_milvus_to_csv.py --output-dir ./exports
  
  # åªå¯¼å‡ºæŒ‡å®šé›†åˆ
  python export_milvus_to_csv.py --collections rag_standard rag_faq
  
  # åŒ…å«å‘é‡å­—æ®µï¼ˆæ–‡ä»¶ä¼šå¾ˆå¤§ï¼‰
  python export_milvus_to_csv.py --include-vector
  
  # æŒ‡å®šæœ¬åœ° Milvus åœ°å€
  python export_milvus_to_csv.py --local-host localhost --local-port 19530
        """
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./milvus_exports",
        help="CSV æ–‡ä»¶è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: ./milvus_exportsï¼‰"
    )
    
    parser.add_argument(
        "--collections",
        nargs="+",
        help="è¦å¯¼å‡ºçš„é›†åˆåç§°ï¼ˆé»˜è®¤: å¯¼å‡ºæ‰€æœ‰é›†åˆï¼‰"
    )
    
    parser.add_argument(
        "--include-vector",
        action="store_true",
        help="åŒ…å«å‘é‡å­—æ®µï¼ˆé€‚åˆç›´æ¥å¯¼å…¥ Attuï¼Œæ–‡ä»¶ä¼šæ›´å¤§ï¼Œé»˜è®¤å…³é—­ï¼‰"
    )
    
    parser.add_argument(
        "--for-import",
        action="store_true",
        help="å¯¼å…¥æ¨¡å¼ï¼šå»æ‰è‡ªå¢ä¸»é”® idï¼Œå­—æ®µé¡ºåºæŒ‰ schema è¾“å‡º"
    )
    
    parser.add_argument(
        "--local-host",
        type=str,
        help=f"æœ¬åœ° Milvus ä¸»æœºåœ°å€ï¼ˆé»˜è®¤: {settings.MILVUS_HOST}ï¼‰"
    )
    
    parser.add_argument(
        "--local-port",
        type=str,
        help=f"æœ¬åœ° Milvus ç«¯å£ï¼ˆé»˜è®¤: {settings.MILVUS_PORT}ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯¼å‡ºå™¨
    exporter = MilvusExporter(
        local_host=args.local_host,
        local_port=args.local_port
    )
    
    try:
        # è¿æ¥
        exporter.connect()
        
        # å¯¼å‡º
        output_dir = Path(args.output_dir)
        exporter.export_all(
            output_dir=output_dir,
            collections=args.collections,
            include_vector=args.include_vector,
            for_import=args.for_import,
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

