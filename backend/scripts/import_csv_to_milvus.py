"""
å°† backend/scripts/milvus_exports ç›®å½•ä¸­çš„ CSV æ–‡ä»¶å¯¼å…¥åˆ°æœ¬åœ° Milvusã€‚

çº¦å®šï¼š
- æ¯ä¸ª CSV æ–‡ä»¶å¯¹åº”ä¸€ä¸ªé›†åˆï¼Œé›†åˆå = æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ï¼Œä¾‹å¦‚ï¼š
    rag_standard.csv  ->  é›†åˆå "rag_standard"
- CSV ç¬¬ä¸€è¡Œä¸ºè¡¨å¤´ï¼Œåç»­è¡Œä¸ºæ•°æ®ã€‚
- å¦‚æœå­˜åœ¨ "id" å­—æ®µï¼Œä¼šåœ¨å¯¼å…¥å‰ç§»é™¤ï¼ˆå› ä¸ºé›†åˆä½¿ç”¨ auto_idï¼‰ã€‚
- å¦‚æœå­˜åœ¨ "vector" å­—æ®µï¼Œåˆ™è®¤ä¸ºæ˜¯ JSON/åˆ—è¡¨å­—ç¬¦ä¸²ï¼Œä¼šè¢«ååºåˆ—åŒ–ä¸º float åˆ—è¡¨åç›´æ¥å†™å…¥ Milvusï¼›
  å¦åˆ™ä¸å¤„ç†å‘é‡ï¼ˆå‡å®šé›†åˆå·²å­˜åœ¨ä¸”ä¼šè‡ªè¡Œå¡«å……ï¼Œæˆ–ä½ å¦æœ‰è„šæœ¬è´Ÿè´£å‘é‡åŒ–ï¼‰ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    # å¯¼å…¥é»˜è®¤ç›®å½•ï¼ˆbackend/scripts/milvus_exportsï¼‰ä¸‹çš„æ‰€æœ‰ CSV
    python import_csv_to_milvus.py

    # åªå¯¼å…¥æŒ‡å®šé›†åˆ
    python import_csv_to_milvus.py --collections rag_standard rag_faq

    # æŒ‡å®š Milvus åœ°å€
    python import_csv_to_milvus.py --local-host localhost --local-port 19530
"""

import sys
import argparse
import csv
import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

from pymilvus import MilvusClient

# åŒæ­¥ MySQL ç›¸å…³
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session

# ä¿è¯èƒ½æ‰¾åˆ° app æ¨¡å—ï¼ˆä¸ crawler.py ä¿æŒä¸€è‡´ï¼Œä» scripts å­ç›®å½•å‘ä¸Šä¸¤çº§åˆ° backendï¼‰
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.config import settings  # noqa: E402
from app.models_db import CrawlTask, CrawlBlock  # noqa: E402


class MilvusImporter:
    """ä» CSV å¯¼å…¥æ•°æ®åˆ°æœ¬åœ° Milvusï¼Œå¹¶åŒæ­¥è®°å½•åˆ° MySQL çš„ CrawlTask/CrawlBlock"""

    def __init__(self, local_host: Optional[str] = None, local_port: Optional[str] = None):
        # é»˜è®¤ä½¿ç”¨ localhostï¼Œæ–¹ä¾¿åœ¨å®¿ä¸»æœºç›´æ¥è¿æ¥ Docker æš´éœ²å‡ºæ¥çš„ Milvus ç«¯å£
        # å¦‚æœä½ åœ¨å®¹å™¨å†…éƒ¨è¿è¡Œè„šæœ¬ï¼Œå¯ä»¥æ˜¾å¼ä¼ å…¥ --local-host milvus-standalone
        self.local_host = local_host or "localhost"
        self.local_port = local_port or settings.MILVUS_PORT
        self.client: Optional[MilvusClient] = None

        # --- MySQL (åŒæ­¥) ---
        # è¿™é‡Œå¤ç”¨ crawler.py ä¸­çš„åŒæ­¥è¿æ¥æ–¹å¼ï¼Œæ–¹ä¾¿è„šæœ¬ç›´æ¥è¿è¡Œ
        # åœ¨å®¿ä¸»æœºï¼ˆWindowsï¼‰ä¸Šç›´æ¥è¿è¡Œè„šæœ¬æ—¶ï¼ŒMySQL æ˜¯é€šè¿‡ Docker æ˜ å°„åˆ°æœ¬æœº 3306 ç«¯å£çš„ï¼Œ
        # è¿™é‡Œæ˜¾å¼ä½¿ç”¨ localhostï¼Œé¿å…ä½¿ç”¨å®¹å™¨å†…éƒ¨çš„ä¸»æœºåï¼ˆå¦‚ "mysql"ï¼‰å¯¼è‡´è§£æå¤±è´¥ã€‚
        sync_db_url = (
            f"mysql+pymysql://{settings.MYSQL_USER}:{settings.MYSQL_PASSWORD}"
            f"@localhost:{settings.MYSQL_PORT}/{settings.MYSQL_DATABASE}"
        )
        self.sync_engine = create_engine(sync_db_url, echo=False, pool_pre_ping=True)
        self.SyncSessionLocal = sessionmaker(
            bind=self.sync_engine, expire_on_commit=False, autoflush=False
        )

        # å·²çŸ¥çš„ RAG / FAQ é›†åˆåï¼Œæ–¹ä¾¿åšä¸€ç‚¹ç‚¹ç‰¹æ®Šå¤„ç†
        self.rag_collections = {
            settings.COLLECTION_STANDARD,
            settings.COLLECTION_KNOWLEDGE,
            settings.COLLECTION_INTERNAL,
            settings.COLLECTION_PERSONAL,
        }
        self.faq_collection = settings.COLLECTION_FAQ

    # ------------------------------------------------------------------ #
    # è¿æ¥
    # ------------------------------------------------------------------ #
    def connect(self) -> None:
        uri = f"http://{self.local_host}:{self.local_port}"
        print("=" * 80)
        print(f"ğŸ”Œ æ­£åœ¨è¿æ¥æœ¬åœ° Milvus: {uri}")
        print("=" * 80)
        self.client = MilvusClient(uri=uri)
        cols = self.client.list_collections()
        print(f"âœ… è¿æ¥æˆåŠŸï¼Œå½“å‰å·²æœ‰é›†åˆ: {cols}")

    # ------------------------------------------------------------------ #
    # CSV è¯»å–ä¸é¢„å¤„ç†
    # ------------------------------------------------------------------ #
    # ------------------------------------------------------------------ #
    # MySQL Session è¾…åŠ©
    # ------------------------------------------------------------------ #
    def _get_db(self) -> Session:
        """è·å–åŒæ­¥æ•°æ®åº“ Session"""
        return self.SyncSessionLocal()

    # ------------------------------------------------------------------ #
    # CSV è¯»å–ä¸é¢„å¤„ç†
    # ------------------------------------------------------------------ #
    def _parse_vector(self, value: str) -> Optional[List[float]]:
        """å°† CSV ä¸­çš„å‘é‡å­—æ®µè§£æä¸º float åˆ—è¡¨"""
        if not value:
            return None
        try:
            # å…¼å®¹ä¸¤ç§æƒ…å†µï¼š
            # 1. JSON å­—ç¬¦ä¸² "[0.1, 0.2, ...]"
            # 2. é€—å·åˆ†éš”çš„ç®€å•åˆ—è¡¨ "0.1,0.2,..."
            v = value.strip()
            if v.startswith("[") and v.endswith("]"):
                arr = json.loads(v)
            else:
                arr = [float(x) for x in v.split(",") if x.strip()]
            return [float(x) for x in arr]
        except Exception as e:
            print(f"   æ— æ³•è§£æå‘é‡å­—æ®µï¼ŒåŸå§‹å€¼å·²è¢«ä¸¢å¼ƒ: {e}")
            return None

    def _prepare_row(self, raw: Dict[str, Any], collection_name: str) -> Dict[str, Any]:
        """æ ¹æ®é›†åˆç±»å‹ï¼ŒæŠŠä¸€è¡Œ CSV è½¬æˆå¯ä»¥æ’å…¥ Milvus çš„å­—å…¸"""
        row = dict(raw)  # æµ…æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸå§‹ dict

        # å»æ‰ idï¼ˆé›†åˆä½¿ç”¨ auto_idï¼‰
        row.pop("id", None)

        # è§£æ vector
        if "vector" in row:
            vec = self._parse_vector(row["vector"])
            if vec is not None:
                row["vector"] = vec
            else:
                row.pop("vector", None)

        # å»æ‰ç©ºå­—ç¬¦ä¸²ï¼Œé¿å…ä¸å¿…è¦çš„è„æ•°æ®
        for k, v in list(row.items()):
            if isinstance(v, str):
                v = v.strip()
                if v == "":
                    row[k] = ""
                else:
                    row[k] = v

        # é’ˆå¯¹å·²çŸ¥ schema åšä¸€ç‚¹å…œåº•å¡«å……ï¼ˆå¯é€‰ï¼‰
        if collection_name in self.rag_collections:
            # ç¡®ä¿ RAG æ–‡æœ¬åº“çš„å‡ ä¸ªå­—æ®µéƒ½æœ‰
            row.setdefault("text", "")
            row.setdefault("source", "")
            row.setdefault("dept_id", "")
            row.setdefault("user_id", "")
        elif collection_name == self.faq_collection:
            # FAQ åº“
            row.setdefault("question", "")
            row.setdefault("answer", "")
            row.setdefault("source", "")

        return row

    def _create_import_task(
        self, db: Session, collection_name: str, csv_path: Path
    ) -> CrawlTask:
        """
        ä¸ºå½“å‰ CSV å¯¼å…¥åˆ›å»ºä¸€ä¸ª CrawlTask è®°å½•ï¼Œæ–¹ä¾¿åç»­ç»Ÿè®¡å’Œå…³è” CrawlBlockã€‚
        """
        task = CrawlTask(
            url=f"import://{collection_name}/{csv_path.name}",
            collection_name=collection_name,
            status="running",
            pages_crawled=0,
            blocks_inserted=0,
            error_message=None,
        )
        db.add(task)
        db.commit()
        db.refresh(task)
        print(f" åˆ›å»ºå¯¼å…¥ä»»åŠ¡ #{task.id}: {task.url}")
        return task

    def _build_crawl_block_from_row(
        self,
        task_id: int,
        collection_name: str,
        prepared_row: Dict[str, Any],
        raw_row: Dict[str, Any],
    ) -> CrawlBlock:
        """
        æ ¹æ®é›†åˆç±»å‹å’Œè¡Œå†…å®¹ï¼Œæ„é€ ä¸€ä¸ª CrawlBlock è®°å½•ã€‚

        ç”±äºå¯¼å…¥çš„æ•°æ®æ¥è‡ª CSV è€Œä¸æ˜¯çœŸå®ç½‘é¡µï¼Œè¿™é‡Œçº¦å®šï¼š
        - url: ä½¿ç”¨ import:// å‰ç¼€æ ‡è®°æ¥æº
        - title/section: æ ¹æ®é›†åˆå’Œå­—æ®µåšä¸€ä¸ªå¤§è‡´å½’ç±»
        - text_preview: æˆªå– text æˆ– question/answer ä½œä¸ºé¢„è§ˆ
        """
        # æ„é€ â€œä¼ª URLâ€ï¼Œæ–¹ä¾¿åŒºåˆ†ä¸åŒé›†åˆå’Œæ¥æº
        base_url = f"import://{collection_name}"
        source = prepared_row.get("source") or raw_row.get("source") or ""
        if source:
            url = f"{base_url}/{source}"
        else:
            url = base_url

        # ç”Ÿæˆé¢„è§ˆæ–‡æœ¬
        text_for_preview = (
            prepared_row.get("text")
            or prepared_row.get("question")
            or prepared_row.get("answer")
            or ""
        )
        text_preview = text_for_preview[:500] if len(text_for_preview) > 500 else text_for_preview

        # ç®€å•çš„ title/section çº¦å®š
        if collection_name == self.faq_collection:
            title = "FAQ é—®ç­”"
            section = "FAQ"
        elif collection_name in self.rag_collections:
            title = "RAG æ–‡æœ¬å—"
            section = collection_name
        else:
            title = "å¯¼å…¥æ–‡æœ¬å—"
            section = collection_name

        crawl_block = CrawlBlock(
            task_id=task_id,
            url=url,
            title=title,
            section=section,
            text_preview=text_preview,
        )
        return crawl_block

    def import_csv_to_collection(
        self, csv_path: Path, collection_name: str, batch_size: int = 1000
    ) -> None:
        """å°†å•ä¸ª CSV æ–‡ä»¶å¯¼å…¥æŒ‡å®šé›†åˆï¼Œå¹¶åŒæ­¥å†™å…¥ MySQL ä¸­çš„ CrawlBlock"""
        assert self.client is not None, "Milvus client æœªè¿æ¥ï¼Œè¯·å…ˆè°ƒç”¨ connect()"
        print("\n" + "-" * 80)
        print(f" æ­£åœ¨å¯¼å…¥ CSV: {csv_path}")
        print(f" ç›®æ ‡é›†åˆ: {collection_name}")

        if not csv_path.exists():
            print(f"   æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {csv_path}")
            return

        # ç®€å•æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        existing_cols = self.client.list_collections()
        if collection_name not in existing_cols:
            print(f"   é›†åˆ {collection_name} ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡å¯¼å…¥ã€‚")
            print("     è¯·å…ˆé€šè¿‡ init_milvus.py æˆ–å…¶å®ƒè„šæœ¬åˆ›å»ºå¯¹åº”é›†åˆã€‚")
            return

        # --- æ‰“å¼€æ•°æ®åº“ Sessionï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå¯¼å…¥ä»»åŠ¡ ---
        db = self._get_db()
        task = self._create_import_task(db, collection_name, csv_path)

        total = 0
        # æŒ‰ crawler.DataIngester.ingest_blocks çš„é£æ ¼ï¼š
        # - ä½¿ç”¨æ‰¹æ¬¡ rows_batch åšæ’å…¥
        # - ä½¿ç”¨ pending_blocks_batch è®°å½•æœ¬æ‰¹æ¬¡å¯¹åº”çš„ CrawlBlock
        # - å…¨é‡ crawl_blocks ä»…ç”¨äºç»Ÿè®¡
        rows_batch: List[Dict[str, Any]] = []
        pending_blocks_batch: List[CrawlBlock] = []
        crawl_blocks: List[CrawlBlock] = []  # ç”¨äºç»Ÿè®¡å’Œæ—¥å¿—

        try:
            with open(csv_path, "r", encoding="utf-8-sig", newline="") as f:
                reader = csv.DictReader(f)
                fieldnames = reader.fieldnames or []
                print(f"   æ£€æµ‹åˆ°å­—æ®µ: {fieldnames}")

                for raw_row in reader:
                    prepared = self._prepare_row(raw_row, collection_name)
                    if not prepared:
                        continue

                    rows_batch.append(prepared)

                    # ä¸ºæœ¬æ¡è®°å½•åˆ›å»ºä¸€ä¸ª CrawlBlockï¼ˆå…ˆä¸å¡« milvus_idï¼‰
                    crawl_block = self._build_crawl_block_from_row(
                        task_id=task.id,
                        collection_name=collection_name,
                        prepared_row=prepared,
                        raw_row=raw_row,
                    )
                    crawl_blocks.append(crawl_block)
                    pending_blocks_batch.append(crawl_block)
                    db.add(crawl_block)

                    if len(rows_batch) >= batch_size:
                        # å…ˆæäº¤ MySQLï¼Œè®©æœ¬æ‰¹æ¬¡ CrawlBlock è·å¾— ID
                        db.commit()

                        # æ’å…¥åˆ° Milvus
                        insert_result = self.client.insert(
                            collection_name=collection_name, data=rows_batch
                        )
                        # insert_result åŒ…å« insert_count/ids ç­‰ä¿¡æ¯ï¼Œå°è¯•æ‹¿å› ID
                        milvus_ids = (
                            insert_result.get("ids")
                            if isinstance(insert_result, dict)
                            else None
                        )

                        if milvus_ids:
                            # æŒ‰æ‰¹æ¬¡é¡ºåºå›å¡«æœ¬æ‰¹æ¬¡å¯¹åº”çš„ CrawlBlock.milvus_id
                            for cb, mid in zip(pending_blocks_batch, milvus_ids):
                                cb.milvus_id = str(mid)
                            db.commit()

                        total += len(rows_batch)
                        print(f"   å·²æ’å…¥ {total} æ¡è®°å½•...")
                        rows_batch = []
                        pending_blocks_batch = []

            # å¤„ç†æœ€åä¸€æ‰¹
            if rows_batch:
                db.commit()
                insert_result = self.client.insert(
                    collection_name=collection_name, data=rows_batch
                )
                milvus_ids = (
                    insert_result.get("ids") if isinstance(insert_result, dict) else None
                )
                if milvus_ids:
                    for cb, mid in zip(pending_blocks_batch, milvus_ids):
                        cb.milvus_id = str(mid)
                    db.commit()
                total += len(rows_batch)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = "completed"
            task.pages_crawled = 1  # è¿™é‡Œæ²¡æœ‰çœŸå®é¡µé¢æ¦‚å¿µï¼Œç®€å•è®°ä¸º 1
            task.blocks_inserted = total
            task.completed_at = datetime.now()
            db.commit()

            print(f"   å¯¼å…¥å®Œæˆï¼Œåˆè®¡æ’å…¥ {total} æ¡è®°å½•åˆ°é›†åˆ {collection_name}")
            print(f"   åŒæ­¥å†™å…¥ MySQL: CrawlTask #{task.id}, CrawlBlock æ•°é‡ {len(crawl_blocks)}")

        except Exception as e:  # noqa: BLE001
            print(f"   å¯¼å…¥ {csv_path} è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            try:
                task.status = "failed"
                task.error_message = str(e)
                task.completed_at = datetime.now()
                db.commit()
            except Exception:
                pass
            raise
        finally:
            db.close()

    # ------------------------------------------------------------------ #
    # æ‰¹é‡å¯¼å…¥
    # ------------------------------------------------------------------ #
    def import_from_dir(
        self,
        input_dir: Path,
        collections: Optional[List[str]] = None,
        batch_size: int = 1000,
    ) -> None:
        """ä»ç›®å½•ä¸­æ‰¹é‡å¯¼å…¥ CSVï¼Œæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰å³é›†åˆå"""
        assert self.client is not None, "Milvus client æœªè¿æ¥ï¼Œè¯·å…ˆè°ƒç”¨ connect()"

        input_dir.mkdir(parents=True, exist_ok=True)

        if collections:
            targets = []
            for name in collections:
                path = input_dir / f"{name}.csv"
                targets.append((path, name))
        else:
            # æ‰«æç›®å½•ä¸‹æ‰€æœ‰ .csv
            targets = []
            for csv_path in sorted(input_dir.glob("*.csv")):
                collection_name = csv_path.stem
                targets.append((csv_path, collection_name))

        if not targets:
            print(f" ç›®å½• {input_dir} ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶")
            return

        print("\n" + "=" * 80)
        print(f" å³å°†å¯¼å…¥ {len(targets)} ä¸ª CSV åˆ° Milvusï¼š")
        for path, col in targets:
            print(f"  - {path.name}  ->  {col}")
        print("=" * 80)

        for idx, (csv_path, collection_name) in enumerate(targets, start=1):
            print(f"\n[{idx}/{len(targets)}] å¤„ç†æ–‡ä»¶: {csv_path.name}")
            self.import_csv_to_collection(csv_path, collection_name, batch_size=batch_size)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="å°† backend/scripts/milvus_exports ä¸‹çš„ CSV å¯¼å…¥åˆ°æœ¬åœ° Milvus é›†åˆä¸­",
    )

    default_dir = Path(__file__).parent / "milvus_exports"
    parser.add_argument(
        "--input-dir",
        type=str,
        default=str(default_dir),
        help=f"CSV è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤: {default_dir}ï¼‰",
    )

    parser.add_argument(
        "--collections",
        nargs="+",
        help="è¦å¯¼å…¥çš„é›†åˆåç§°ï¼ˆé»˜è®¤ä¸ºç›®å½•ä¸‹æ‰€æœ‰ CSV æ–‡ä»¶åï¼‰",
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="æ‰¹é‡æ’å…¥å¤§å°ï¼ˆé»˜è®¤: 1000ï¼‰",
    )

    parser.add_argument(
        "--local-host",
        type=str,
        help=f"æœ¬åœ° Milvus ä¸»æœºåœ°å€ï¼ˆé»˜è®¤: {settings.MILVUS_HOST}ï¼‰",
    )

    parser.add_argument(
        "--local-port",
        type=str,
        help=f"æœ¬åœ° Milvus ç«¯å£ï¼ˆé»˜è®¤: {settings.MILVUS_PORT}ï¼‰",
    )

    args = parser.parse_args()

    importer = MilvusImporter(
        local_host=args.local_host,
        local_port=args.local_port,
    )

    try:
        importer.connect()
        input_dir = Path(args.input_dir)
        importer.import_from_dir(
            input_dir=input_dir,
            collections=args.collections,
            batch_size=args.batch_size,
        )
    except KeyboardInterrupt:
        print("\n ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:  # noqa: BLE001
        print(f"\n å¯¼å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()


